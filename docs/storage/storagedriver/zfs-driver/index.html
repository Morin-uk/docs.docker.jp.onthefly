<p>ZFS is a next generation filesystem that supports many advanced storage
technologies such as volume management, snapshots, checksumming, compression and
deduplication, replication and more.</p>

<p>It was created by Sun Microsystems (now Oracle Corporation) and is open sourced
under the CDDL license. Due to licensing incompatibilities between the CDDL and
GPL, ZFS cannot be shipped as part of the mainline Linux kernel. However, the
ZFS On Linux (ZoL) project provides an out-of-tree kernel module and userspace
tools which can be installed separately.</p>

<p>The ZFS on Linux (ZoL) port is healthy and maturing. However, at this point in
time it is not recommended to use the <code class="language-plaintext highlighter-rouge">zfs</code> Docker storage driver for production
use unless you have substantial experience with ZFS on Linux.</p>

<blockquote>
  <p><strong><em>Note</em></strong>: There is also a FUSE implementation of ZFS on the Linux platform.
This is not recommended. The native ZFS driver (ZoL) is more tested, more
performant, and is more widely used. The remainder  of this document refers
to the native ZoL port.</p>
</blockquote>

<h2 id="prerequisites">Prerequisites</h2>

<ul>
  <li>ZFS requires one or more dedicated block devices, preferably solid-state
drives (SSDs).</li>
  <li>ZFS is only supported on Docker Engine - Community with Ubuntu 14.04 or higher, with the <code class="language-plaintext highlighter-rouge">zfs</code>
package (16.04 and higher) or <code class="language-plaintext highlighter-rouge">zfs-native</code> and <code class="language-plaintext highlighter-rouge">ubuntu-zfs</code> packages (14.04)
installed.
    <ul>
      <li>For Ubuntu 14.04, you need to enable a supplemental package repository
<code class="language-plaintext highlighter-rouge">ppa:zfs-native/stable</code> before you can install the package. See
<a href="https://launchpad.net/~zfs-native/+archive/ubuntu/stable" target="_blank" rel="noopener" class="_">https://launchpad.net/~zfs-native/+archive/ubuntu/stable</a>
for instructions.</li>
    </ul>
  </li>
  <li>ZFS is not supported on Docker EE or CS-Engine, or any other Linux platforms.</li>
  <li>The <code class="language-plaintext highlighter-rouge">/var/lib/docker/</code> directory must be mounted on a ZFS-formatted
filesystem.</li>
  <li>Changing the storage driver makes any containers you have already
created inaccessible on the local system. Use <code class="language-plaintext highlighter-rouge">docker save</code> to save containers,
and push existing images to Docker Hub or a private repository, so that you
do not need to re-create them later.</li>
</ul>

<blockquote>
  <p><strong>Note</strong>: There is no need to use <code class="language-plaintext highlighter-rouge">MountFlags=slave</code> with Docker Engine 18.09 or
later because <code class="language-plaintext highlighter-rouge">dockerd</code> and <code class="language-plaintext highlighter-rouge">containerd</code> are in different mount namespaces.</p>
</blockquote>

<h2 id="configure-docker-with-the-zfs-storage-driver">Configure Docker with the <code class="language-plaintext highlighter-rouge">zfs</code> storage driver</h2>

<ol>
  <li>
    <p>Stop Docker.</p>
  </li>
  <li>
    <p>Copy the contents of <code class="language-plaintext highlighter-rouge">/var/lib/docker/</code> to <code class="language-plaintext highlighter-rouge">/var/lib/docker.bk</code> and remove
the contents of <code class="language-plaintext highlighter-rouge">/var/lib/docker/</code>.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo cp</span> <span class="nt">-au</span> /var/lib/docker /var/lib/docker.bk

<span class="nv">$ </span><span class="nb">sudo rm</span> <span class="nt">-rf</span> /var/lib/docker/<span class="k">*</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Create a new <code class="language-plaintext highlighter-rouge">zpool</code> on your dedicated block device or devices, and mount it
into <code class="language-plaintext highlighter-rouge">/var/lib/docker/</code>. Be sure you
have specified the correct devices, because this is a destructive operation.
This example adds two devices to the pool.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>zpool create <span class="nt">-f</span> zpool-docker <span class="nt">-m</span> /var/lib/docker /dev/xvdf /dev/xvdg
</code></pre></div>    </div>

    <p>The command creates the <code class="language-plaintext highlighter-rouge">zpool</code> and names it <code class="language-plaintext highlighter-rouge">zpool-docker</code>. The name is for
display purposes only, and you can use a different name. Check that the pool
was created and mounted correctly using <code class="language-plaintext highlighter-rouge">zfs list</code>.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>zfs list

NAME           USED  AVAIL  REFER  MOUNTPOINT
zpool-docker    55K  96.4G    19K  /var/lib/docker
</code></pre></div>    </div>
  </li>
  <li>
    <p>Configure Docker to use <code class="language-plaintext highlighter-rouge">zfs</code>. Edit <code class="language-plaintext highlighter-rouge">/etc/docker/daemon.json</code> and set the
<code class="language-plaintext highlighter-rouge">storage-driver</code> to <code class="language-plaintext highlighter-rouge">zfs</code>. If the file was empty before, it should now look
like this:</p>

    <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"storage-driver"</span><span class="p">:</span><span class="w"> </span><span class="s2">"zfs"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div>    </div>

    <p>Save and close the file.</p>
  </li>
  <li>
    <p>Start Docker. Use <code class="language-plaintext highlighter-rouge">docker info</code> to verify that the storage driver is <code class="language-plaintext highlighter-rouge">zfs</code>.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>docker info
  Containers: 0
   Running: 0
   Paused: 0
   Stopped: 0
  Images: 0
  Server Version: 17.03.1-ce
  Storage Driver: zfs
   Zpool: zpool-docker
   Zpool Health: ONLINE
   Parent Dataset: zpool-docker
   Space Used By Parent: 249856
   Space Available: 103498395648
   Parent Quota: no
   Compression: off
&lt;output truncated&gt;
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="manage-zfs">Manage <code class="language-plaintext highlighter-rouge">zfs</code></h2>

<h3 id="increase-capacity-on-a-running-device">Increase capacity on a running device</h3>

<p>To increase the size of the <code class="language-plaintext highlighter-rouge">zpool</code>, you need to add a dedicated block device to
the Docker host, and then add it to the <code class="language-plaintext highlighter-rouge">zpool</code> using the <code class="language-plaintext highlighter-rouge">zpool add</code> command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>zpool add zpool-docker /dev/xvdh
</code></pre></div></div>

<h3 id="limit-a-containers-writable-storage-quota">Limit a container’s writable storage quota</h3>

<p>If you want to implement a quota on a per-image/dataset basis, you can set the
<code class="language-plaintext highlighter-rouge">size</code> storage option to limit the amount of space a single container can use
for its writable layer.</p>

<p>Edit <code class="language-plaintext highlighter-rouge">/etc/docker/daemon.json</code> and add the following:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"storage-driver"</span><span class="p">:</span><span class="w"> </span><span class="s2">"zfs"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"storage-opts"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"size=256M"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>See all storage options for each storage driver in the
<a href="/engine/reference/commandline/dockerd/#storage-driver-options">daemon reference documentation</a></p>

<p>Save and close the file, and restart Docker.</p>

<h2 id="how-the-zfs-storage-driver-works">How the <code class="language-plaintext highlighter-rouge">zfs</code> storage driver works</h2>

<p>ZFS uses the following objects:</p>

<ul>
  <li><strong>filesystems</strong>: thinly provisioned, with space allocated from the <code class="language-plaintext highlighter-rouge">zpool</code> on
demand.</li>
  <li><strong>snapshots</strong>: read-only space-efficient point-in-time copies of filesystems.</li>
  <li><strong>clones</strong>: Read-write copies of snapshots. Used for storing the differences
from the previous layer.</li>
</ul>

<p>The process of creating a clone:</p>

<p><img src="/docs.docker.jp.onthefly/storage/storagedriver/images/zfs_clones.jpg" alt="zfs snapshots and clones" /></p>

<ol>
  <li>A read-only snapshot is created from the filesystem.</li>
  <li>A writable clone is created from the snapshot. This contains any differences
from the parent layer.</li>
</ol>

<p>Filesystems, snapshots, and clones all allocate space from the underlying
<code class="language-plaintext highlighter-rouge">zpool</code>.</p>

<h3 id="image-and-container-layers-on-disk">Image and container layers on-disk</h3>

<p>Each running container’s unified filesystem is mounted on a mount point in
<code class="language-plaintext highlighter-rouge">/var/lib/docker/zfs/graph/</code>. Continue reading for an explanation of how the
unified filesystem is composed.</p>

<h3 id="image-layering-and-sharing">Image layering and sharing</h3>

<p>The base layer of an image is a ZFS filesystem. Each child layer is a ZFS clone
based on a ZFS snapshot of the layer below it. A container is a ZFS clone based
on a ZFS Snapshot of the top layer of the image it’s created from.</p>

<p>The diagram below shows how this is put together with a running container based
on a two-layer image.</p>

<p><img src="/docs.docker.jp.onthefly/storage/storagedriver/images/zfs_zpool.jpg" alt="zfs pool for Docker container" /></p>

<p>When you start a container, the following steps happen in order:</p>

<ol>
  <li>
    <p>The base layer of the image exists on the Docker host as a ZFS filesystem.</p>
  </li>
  <li>
    <p>Additional image layers are clones of the dataset hosting the image layer
directly below it.</p>

    <p>In the diagram, “Layer 1” is added by taking a ZFS snapshot of the base
layer and then creating a clone from that snapshot. The clone is writable and
consumes space on-demand from the zpool. The snapshot is read-only,
maintaining the base layer as an immutable object.</p>
  </li>
  <li>
    <p>When the container is launched, a writable layer is added above the image.</p>

    <p>In the diagram, the container’s read-write layer is created by making
a snapshot of the top layer of the image (Layer 1) and creating a clone from
that snapshot.</p>
  </li>
  <li>
    <p>As the container modifies the contents of its writable layer, space is
allocated for the blocks that are changed. By default, these blocks are
128k.</p>
  </li>
</ol>

<h2 id="how-container-reads-and-writes-work-with-zfs">How container reads and writes work with <code class="language-plaintext highlighter-rouge">zfs</code></h2>

<h3 id="reading-files">Reading files</h3>

<p>Each container’s writable layer is a ZFS clone which shares all its data with
the dataset it was created from (the snapshots of its parent layers). Read
operations are fast, even if the data being read is from a deep layer.
This diagram illustrates how block sharing works:</p>

<p><img src="/docs.docker.jp.onthefly/storage/storagedriver/images/zpool_blocks.jpg" alt="zfs block sharing" /></p>

<h3 id="writing-files">Writing files</h3>

<p><strong>Writing a new file</strong>: space is allocated on demand from the underlying <code class="language-plaintext highlighter-rouge">zpool</code>
and the blocks are written directly into the container’s writable layer.</p>

<p><strong>Modifying an existing file</strong>: space is allocated only for the changed blocks,
and those blocks are written into the container’s writable layer using a
copy-on-write (CoW) strategy. This minimizes the size of the layer and increases
write performance.</p>

<p><strong>Deleting a file or directory</strong>:</p>
<ul>
  <li>When you delete a file or directory that exists in a lower layer, the ZFS
driver masks the existence of the file or directory in the container’s
writable layer, even though the file or directory still exists in the lower
read-only layers.</li>
  <li>If you create and then delete a file or directory within the container’s
writable layer, the blocks are reclaimed by the <code class="language-plaintext highlighter-rouge">zpool</code>.</li>
</ul>

<h2 id="zfs-and-docker-performance">ZFS and Docker performance</h2>

<p>There are several factors that influence the performance of Docker using the
<code class="language-plaintext highlighter-rouge">zfs</code> storage driver.</p>

<ul>
  <li>
    <p><strong>Memory</strong>: Memory has a major impact on ZFS performance. ZFS was originally
designed for large enterprise-grade servers with a large amount of memory.</p>
  </li>
  <li>
    <p><strong>ZFS Features</strong>: ZFS includes a de-duplication feature. Using this feature
may save disk space, but uses a large amount of memory. It is recommended that
you disable this feature for the <code class="language-plaintext highlighter-rouge">zpool</code> you are using with Docker, unless you
are using SAN, NAS, or other hardware RAID technologies.</p>
  </li>
  <li>
    <p><strong>ZFS Caching</strong>: ZFS caches disk blocks in a memory structure called the
adaptive replacement cache (ARC). The <em>Single Copy ARC</em> feature of ZFS allows
a single cached copy of a block to be shared by multiple clones of a
With this feature, multiple running containers can share a single copy of a
cached block. This feature makes ZFS a good option for PaaS and other
high-density use cases.</p>
  </li>
  <li>
    <p><strong>Fragmentation</strong>: Fragmentation is a natural byproduct of copy-on-write
filesystems like ZFS. ZFS mitigates this by using a small block size of 128k.
The ZFS intent log (ZIL) and the coalescing of writes (delayed writes) also
help to reduce fragmentation. You can monitor fragmentation using
<code class="language-plaintext highlighter-rouge">zpool status</code>. However, there is no way to defragment ZFS without reformatting
and restoring the filesystem.</p>
  </li>
  <li>
    <p><strong>Use the native ZFS driver for Linux</strong>: The ZFS FUSE implementation is not
recommended, due to poor performance.</p>
  </li>
</ul>

<h3 id="performance-best-practices">Performance best practices</h3>

<ul>
  <li>
    <p><strong>Use fast storage</strong>:  Solid-state drives (SSDs) provide faster reads and
writes than spinning disks.</p>
  </li>
  <li>
    <p><strong>Use volumes for write-heavy workloads</strong>: Volumes provide the best and most
predictable performance for write-heavy workloads. This is because they bypass
the storage driver and do not incur any of the potential overheads introduced
by thin provisioning and copy-on-write. Volumes have other benefits, such as
allowing you to share data among containers and persisting even when no
running container is using them.</p>
  </li>
</ul>

