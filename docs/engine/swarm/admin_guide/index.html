
<p>Docker Engine において Swarm を起動している場合、<strong>マネージャーノード</strong> が主要な要素であり、Swarm を管理し、Swarm の状態を保持します。
ですからマネージャーノードの主な機能を理解しておくことが重要です。
これを知ることによって、適切なデプロイと Swarm の保守が可能になります。</p>

<p><a href="/docs.docker.jp.onthefly/engine/swarm/how-swarm-mode-works/nodes/">ノードの動作</a> を参照して、Docker Swarm モードの概要や、マネージャーノード、ワーカーノードの違いについて理解しておいてください。</p>

<h2 id="operate-manager-nodes-in-a-swarm">Swarm におけるマネージャーノードの操作</h2>

<p>Swarm マネージャーノードは <a href="/docs.docker.jp.onthefly/engine/swarm/raft/">Raft 合意アルゴリズム</a>（Raft Consensus Algorithm）を利用して、Swarm の状態を管理しています。
Swarm を管理する上では、Raft の一般的な考え方をいくつか理解しておくだけで十分です。</p>

<p>マネージャーノードの数には制限がありません。
マネージャーノードをいくつに設定するかは、性能とフォールトトレランスとのトレードオフです。
マネージャーノードを Swarm に追加すれば、障害には強くなります。
しかしマネージャーノードが増えれば書き込み性能は低下します。
Swarm 状態の更新処理を、より多くのノードにおいて受けつけなければならないからです。
これはつまりネットワークのラウンドトリップトラフィックを意味します。</p>

<p>Raft では Swarm において提案される更新処理、たとえばノード追加や削除といった処理に対して、多数のマネージャーが承認する必要があります。
多数というのは quorum とも呼ばれます。
メンバーに関わる処理に関しては、状態レプリケーションと同様の制約が適用されます。</p>

<h3 id="maintain-the-quorum-of-managers">マネージャーの quorum 管理</h3>

<p>Swarm がマネージャーの quorum（多数票）を得られなかった場合、この Swarm は管理タスクを実行できません。
Swarm のマネージャーを複数にする場合は、必ず 3 つ以上にしてください。
quorum を維持するためには、マネージャーが多数いるという状態がなければなりません。
マネージャー数は奇数にすることが推奨されています。
仮にもう 1 つ加えて偶数にしてしまうと quorum を簡単には取り扱えなくなります。
たとえばマネージャー数が 3 つあるいは 4 つである場合、1 つのマネージャーを失っても quorum を維持することができます。これが 5 つあるいは 6 つであれば 2 つまでなら失っても維持が可能です。</p>

<p>仮に Swarm がマネージャーの quorum を維持できなくなったとしても、既存のワーカーノード上での Swarm タスクは処理続行されます。
ただし Swarm ノードの追加、更新、削除はできなくなります。
さらに新規や既存のタスクに対する起動、停止、移動、更新ができなくなります。</p>

<p>マネージャーが quorum を失った場合の対処方法については <a href="#recover-from-losing-the-quorum">quorum を失ってからの回復</a> を参照してください。</p>

<h2 id="configure-the-manager-to-advertise-on-a-static-ip-address">マネージャーの通知アドレスへのスタティック IP アドレス設定</h2>

<p>Swarm を初期化する際には<code class="language-plaintext highlighter-rouge">--advertise-addr</code>フラグの指定により、Swarm ないの別のマネージャーノードに対して自身のアドレスを通知（advertise）する必要があります。
詳しくは<a href="/docs.docker.jp.onthefly/engine/swarm/swarm-mode/#configure-the-advertise-address">Swarm モードによる Docker Engine の実行</a> を参照してください。
マネージャーノードはそのインフラストラクチャーの中でも安定した構成要素であることが求められます。
したがって通知アドレスには <strong>固定 IP アドレス</strong> を指定すべきであり、これによってマシンの再起動を行っても Swarm が不安定にならないようにする必要があります。</p>

<p>Swarm 全体が再起動してマネージャーノードが次々に新たな IP アドレスを取得したら、どのノードもマネージャーに接続できなくなります。
そして各ノードは、お互いに古い IP アドレスを使ってアクセスしようと試みるために Swarm がハングしてしまいます。</p>

<p>動的 IP アドレスはワーカーノードに対してなら用いてもかまいません。</p>

<h2 id="add-manager-nodes-for-fault-tolerance">耐障害性のためのマネージャーノード追加</h2>

<p>Swarm においてノード障害に対処するには、マネージャーノード数を奇数に保つことが必要です。
マネージャーを奇数にしておくと、ネットワークパーティション分割の処理が発生した際に、そのパーティンションがちょうど 2 つに分かれたのであれば、quorum がリクエストを処理し続けられる可能性が確実に高まります。
なおネットワークパーティションが 3 つ以上に分かれた場合には quorum の機能維持は保証されません。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Swarm サイズ</th>
      <th style="text-align: center">多数票</th>
      <th style="text-align: center">耐障害性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>3</strong></td>
      <td style="text-align: center">2</td>
      <td style="text-align: center"><strong>1</strong></td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>5</strong></td>
      <td style="text-align: center">3</td>
      <td style="text-align: center"><strong>2</strong></td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>7</strong></td>
      <td style="text-align: center">4</td>
      <td style="text-align: center"><strong>3</strong></td>
    </tr>
    <tr>
      <td style="text-align: center">8</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">3</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>9</strong></td>
      <td style="text-align: center">5</td>
      <td style="text-align: center"><strong>4</strong></td>
    </tr>
  </tbody>
</table>

<p>たとえば <strong>5 つのノード</strong> からなる Swarm において <strong>3 つのノード</strong> を失ったら、quorum は 1 つもありません。
こうなるとノードの追加や削除はできなくなります。
利用不能になったマネージャーノードを復旧するか、障害回復のためのコマンドを使って Swarm を回復させなければなりません。
<a href="#recover-from-disaster">障害からの復旧</a> を参照してください。</p>

<p>Swarm をスケールダウンして単一のマネージャーノードにすることは可能ですが、最後に残るマネージャーノードを降格させることはできません。
最低 1 つのマネージャーを残しておかないと、Swarm にアクセスできなくなり、リクエストを処理することができなくなります。
ただし単一のマネージャーノードにスケールダウンすることは、安全な操作とは言えないため推奨されません。
その最後のノードが降格処理の際に、図らずも Swarm から離れてしまった場合、Swarm を操作することはできなくなります。
これを復旧するには、ノードを再起動するか、あるいは<code class="language-plaintext highlighter-rouge">--force-new-cluster</code>を使って再起動することが必要になります。</p>

<p>Swarm へのメンバー管理には、サブシステム<code class="language-plaintext highlighter-rouge">docker swarm</code>および<code class="language-plaintext highlighter-rouge">docker node</code>を利用します。
ワーカーノードの追加方法や、マネージャーノードへの昇格方法についての詳細は、<a href="/docs.docker.jp.onthefly/engine/swarm/join-nodes/">Swarm へのノード追加</a> を参照してください。</p>

<h3 id="distribute-manager-nodes">分散マネージャーノード</h3>

<p>マネージャーノードを奇数に維持することに加えて、マネージャーの配置に関してはデータセンターのトポロジーに配慮することも必要です。
最適なフォールトトレランスを実現するには、最低でも 3 つのアベイラビリティーゾーン（availability-zones）にマネージャーノードを分散して、マシン全体の障害へ対処したり、一般的なメンテナンス計画を立てたりすることが求められます。
いずれかのゾーンにおいて障害が発生したとしても、Swarm はマネージャーノードの quorum を維持することが可能であり、リクエストを処理し負荷を分散することが可能になります。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Swarm マネージャーノード</th>
      <th style="text-align: center">再パーティション化 (3 つのアベイラビリティーゾーン)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">1-1-1</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">2-2-1</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: center">3-2-2</td>
    </tr>
    <tr>
      <td style="text-align: center">9</td>
      <td style="text-align: center">3-3-3</td>
    </tr>
  </tbody>
</table>

<h3 id="run-manager-only-nodes">マネージャーノードのみの実行</h3>

<p>マネージャーノードは、デフォルトでワーカーノードとしても動作します。
つまりスケジューラーはマネージャーノードに対してもタスクを割り振るということです。
小さくて重大性の高くない Swarm の場合、マネージャーノードへのタスクの割り当ては比較的低リスクです。
サービスのスケジュールにあたって <strong>CPU</strong> や <strong>メモリ</strong> に対する <strong>リソース制約</strong> を適切に行っておけば十分です。</p>

<p>しかしマネージャーノードでは Raft 合意アルゴリズム（Raft consensus algorithm）を利用して、一貫した方法によりデータ複製を行っています。
そのためリソースが枯渇することは大いに問題となります。
したがって Swarm のハートビートやリーダー選択のような Swarm 操作に支障をきたすような処理は、マネージャーに行わせないことが必要です。</p>

<p>マネージャーノード処理への影響を避けるためには、マネージャーノードを排出（drain）させて、ワーカーノードとしては利用できなくします。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker node update <span class="nt">--availability</span> drain &lt;NODE&gt;
</code></pre></div></div>

<p>ノードを排出したらそのノード上の実行タスクは、スケジューラーが Swarm 内の別のワーカーノードに再割り当てします。
そしてスケジューラーは、そのノードへのタスク割り当てを行わなくなります。</p>

<h2 id="add-worker-nodes-for-load-balancing">負荷分散のためのワーカーノード追加</h2>

<p><a href="/docs.docker.jp.onthefly/engine/swarm/join-nodes/">Swarm へのノード参加</a> を行うことで、Swarm 内の負荷を分散することができます。
複製されたサービスタスクは Swarm 全体にわたって分散されますが、それはどれくらいの時間であっても、ワーカーノードがサービス条件を満たしている限り行われます。
特定タイプのノード上でしかサービスを実行できない制限があるとします。
たとえば所定の CPU 数やメモリ容量が要求されるような場合です。
そういった条件を満たさないワーカーノード上では、タスクが実行できない点を覚えておいてください。</p>

<h2 id="swarm-の健康状態の監視">Swarm の健康状態の監視</h2>

<p>マネージャーノードの健康状態（health）を監視するには、Docker の<code class="language-plaintext highlighter-rouge">nodes</code> API を使い、HTTP のエンドポイント<code class="language-plaintext highlighter-rouge">/nodes</code>から JSON 書式により確認することができます。
詳しくは <a href="/engine/api/v1.25/#tag/Node">nodes API のドキュメント</a> を参照してください。</p>

<p>コマンドラインからは<code class="language-plaintext highlighter-rouge">docker node inspect &lt;id-node&gt;</code>を実行して、ノードを調べることができます。
たとえばノードに、マネージャーとしての到達性能（reachability）があるかどうかを調べるには、以下を実行します。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker node inspect manager1 <span class="nt">--format</span> <span class="s2">"{{ .ManagerStatus.Reachability }}"</span>
reachable
</code></pre></div></div>

<p>ワーカーノードとしてタスクの受け入れが可能であるかを確認するには、以下を実行します。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker node inspect manager1 <span class="nt">--format</span> <span class="s2">"{{ .Status.State }}"</span>
ready
</code></pre></div></div>

<p>上のコマンドから<code class="language-plaintext highlighter-rouge">manager1</code>は、マネージャーとしては<code class="language-plaintext highlighter-rouge">reachable</code>（到達可能）であり、ワーカーとしては<code class="language-plaintext highlighter-rouge">ready</code>（受け入れ可能）であることがわかります。</p>

<p>健康状態が<code class="language-plaintext highlighter-rouge">unreachable</code>（到達不能）であるのは、このマネージャーノードが他のマネージャーノードから到達できないことを意味します。
この場合、到達不能なマネージャーノードは復旧させる措置が必要になります。</p>

<ul>
  <li>デーモンを再起動し、マネージャーが到達可能に戻るかどうかを確認します。</li>
  <li>マシンを再起動します。</li>
  <li>上の 2 つの再起動を行っても動作しない場合は、新たなマネージャーノードを追加するか、ワーカーノードをマネージャーに昇格させます。
そしてマネージャーから障害ノードのエントリを削除する必要があるので、<code class="language-plaintext highlighter-rouge">docker node demote &lt;NODE&gt;</code>と<code class="language-plaintext highlighter-rouge">docker node rm &lt;id-node&gt;</code>を実行します。</li>
</ul>

<p>上とは別に、Swarm の健康状態の概要を確認するには、マネージャーノードから以下のように<code class="language-plaintext highlighter-rouge">docker node ls</code>を実行します。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
docker node <span class="nb">ls
</span>ID                           HOSTNAME  MEMBERSHIP  STATUS  AVAILABILITY  MANAGER STATUS
1mhtdwhvsgr3c26xxbnzdc3yp    node05    Accepted    Ready   Active
516pacagkqp2xc3fk9t1dhjor    node02    Accepted    Ready   Active        Reachable
9ifojw8of78kkusuc4a6c23fx <span class="k">*</span>  node01    Accepted    Ready   Active        Leader
ax11wdpwrrb6db3mfjydscgk7    node04    Accepted    Ready   Active
bb1nrq2cswhtbg4mrsqnlx1ck    node03    Accepted    Ready   Active        Reachable
di9wxgz8dtuh9d2hn089ecqkf    node06    Accepted    Ready   Active
</code></pre></div></div>

<h2 id="troubleshoot-a-manager-node">マネージャーノードのトラブルシューティング</h2>

<p>マネージャーノードの<code class="language-plaintext highlighter-rouge">raft</code>ディレクトリを、別ノードのものからコピーして再起動するようなことはやめてください。
そのデータディレクトリはノード ID に固有のものです。
そしてノード ID が利用されるのは Swarm に参加するときだけです。
ノード ID 空間は全体にわたってユニークでなければなりません。</p>

<p>クラスターに対してマネージャーノードを再参加させるには、以下を行います。</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">docker node demote &lt;NODE&gt;</code>を実行して、そのノードをワーカーに降格させます。</li>
  <li><code class="language-plaintext highlighter-rouge">docker node rm &lt;NODE&gt;</code>を実行して、そのノードを Swarm から削除します。</li>
  <li><code class="language-plaintext highlighter-rouge">docker swarm join</code>を実行して、そのノードを Swarm に再度、新たな状態により参加させます。</li>
</ol>

<p>Swarm へのマネージャーノードの参加に関する詳細は <a href="/docs.docker.jp.onthefly/engine/swarm/join-nodes/">Swarm へのノード参加</a> を参照してください。</p>

<h2 id="forcibly-remove-a-node">ノードの強制削除</h2>

<p>たいていの場合、<code class="language-plaintext highlighter-rouge">docker node rm</code>コマンドを使って Swarm からノードを削除するには、あらかじめそのノードを停止させておくべきです。
ノードが到達不能、無反応、障害発生といった状態になったら、ノードを停止させなくても、<code class="language-plaintext highlighter-rouge">--force</code>フラグを使って強制的にノードを削除することができます。
たとえば<code class="language-plaintext highlighter-rouge">node9</code>に障害が発生したら以下を実行します。</p>

<pre><code class="language-none">$ docker node rm node9

Error response from daemon: rpc error: code = 9 desc = node node9 is not down and can't be removed

$ docker node rm --force node9

Node node9 removed from swarm
</code></pre>

<p>マネージャーノードを強制的に削除する場合は、その前にワーカーノードへ降格させておかなければなりません。
マネージャーノードを降格あるいは削除した場合、マネージャーノードは常に奇数にしておくべきことを忘れないでください。</p>

<h2 id="back-up-the-swarm">Swarm のバックアップ</h2>

<p>マネージャーノードは、Swarm の状態とマネージャーログを<code class="language-plaintext highlighter-rouge">/var/lib/docker/swarm/</code>ディレクトリに保存しています。
バージョン 1.13 またはそれ以降においては、そのディレクトリに Raft ログの暗号化に用いる鍵も含まれています。
この鍵がない状態では、Swarm を復元することはできません。</p>

<p>Swarm のバックアップはどのマネージャーからでも行うことができます。
バックアップは以下の手順で行います。</p>

<ol>
  <li>
    <p>Swarm のオートロック機能が有効である場合、バックアップから Swarm を復元するためには解除鍵（unlock key）が必要です。
解除鍵を得たら、安全な場所に保存しておいてください。
内容がよくわからない場合は、<a href="/docs.docker.jp.onthefly/engine/swarm/swarm_manager_locking/">Swarm のロックと暗号鍵の保護</a> を参照してください。</p>
  </li>
  <li>
    <p>データのバックアップを取る前に、マネージャーノード上から Docker を停止します。
これによってバックアップ中にデータが変更されることがなくなります。
マネージャーが稼動中にバックアップを取る（「ホット」バックアップを取る）ことも可能ですが、これはお勧めしません。
バックアップを復元する際に、予期しない事態となる場合があります。
マネージャーが停止していても、他のノードが Swarm データを生成していると、そのデータはバックアップには含まれません。</p>

    <blockquote>
      <p>メモ</p>

      <p>Swarm マネージャーの quorum は維持することを忘れないでください。
マネージャーを停止している最中には、さらに別のノードが失われると Swarm の quorum を失う危険性が増します。
実行するマネージャー数にはトレードオフがあります。
バックアップを取る際には常にマネージャーを停止させるのであれば、Swarm のマネージャー数を 5 つとすることを考えてみてください。
そうしておくと、バックアップ中に別のマネージャーが失われても、サービスを支障なく実行することができます。</p>
    </blockquote>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">/var/lib/docker/swarm</code>ディレクトリ全体のバックアップを取ります。</p>
  </li>
  <li>
    <p>マネージャーを再起動します。</p>
  </li>
</ol>

<p>復元方法については <a href="#restore-from-a-backup">バックアップからの復元</a> を参照してください。</p>

<h2 id="recover-from-disaster">障害からの復旧</h2>

<h3 id="restore-from-a-backup">バックアップからの復元</h3>

<p><a href="#back-up-the-swarm">Swarm のバックアップ</a> において説明したように Swarm のバックアップを取った後は、以下の手順に従って、新たな Swarm に対してデータ復元を行います。</p>

<ol>
  <li>
    <p>復元対象とする Swarm の対象ホストマシンを使って、Docker を停止させます。</p>
  </li>
  <li>
    <p>新たな Swarm の<code class="language-plaintext highlighter-rouge">/var/lib/docker/swarm</code>ディレクトリ内容を削除します。</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">/var/lib/docker/swarm</code>ディレクトリ内を、バックアップ内容に置き換えます。</p>

    <blockquote>
      <p>メモ</p>

      <p>新しいノードはディスク上のストレージに対して、古いときと同じ暗号鍵を用います。
この時点でディスク上のストレージに対する暗号鍵を変更することはできません。</p>

      <p>Swarm のオートロック機能が有効である場合、解除鍵についても古い Swarm のときと同じ鍵です。
この解除鍵は Swarm を復元するために必要となります。</p>
    </blockquote>
  </li>
  <li>
    <p>新たなノード上から Docker を起動させます。
必要に応じて Swarm のロック解除を行います。
以下のコマンドを使って Swarm を再初期化します。
これを行うのは、このノードが古い Swarm に属していたノードに接続しに行かないようにするためです。
もっとも古いノードは、すでに存在していないはずです。</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker swarm init <span class="nt">--force-new-cluster</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Swarm の状態が思っているとおりかどうかを確認します。
確認としてはアプリケーションに固有のテストを行うこともでき、あるいは単に<code class="language-plaintext highlighter-rouge">docker service ls</code>の出力を確認するのでもかまいません。
これによってサービスが思い通りに存在しているかどうかを確認します。</p>
  </li>
  <li>
    <p>オートロック機能を利用している場合は <a href="/docs.docker.jp.onthefly/engine/swarm/swarm_manager_locking/#rotate-the-unlock-key">解除鍵のローテート</a> を行います。</p>
  </li>
  <li>
    <p>新たな Swarm の収容性能を実現するために、マネージャーノードやワーカーノードを追加します。</p>
  </li>
  <li>
    <p>新たな Swarm に対して、以前からのバックアップ計画を復活させます。</p>
  </li>
</ol>

<h3 id="recover-from-losing-the-quorum">quorum を失ってからの回復</h3>

<p>Swarm というものには障害に対しての耐久性があります。
いくつものノードが一時的な障害（マシン再起動やその際のクラッシュなど）や一時的エラーを起こしても、Swarm は回復できます。
ただし quorum を失っている場合、Swarm は自動的に復旧できません。
既存のワーカーノード上のタスクは動作をし続けますが、管理タスクは実行不能になります。
たとえばサービスのスケーリング、更新、ノードの追加や削除などです。
復旧する最良の策は、失われたマネージャーノードをオンライン復旧させることです。
これが無理である場合、Swarm 復旧のための手段がいくつかあるので、読み進めてください。</p>

<p>Swarm に<code class="language-plaintext highlighter-rouge">N</code>個のマネージャーがある場合に、マネージャーノードの quorum は常に利用可能でなければなりません。
たとえば 5 つのマネージャーからなる Swarm の場合、最低でも 3 つのノードは動作状態であって、互いに通信できなければなりません。
言い換えると、Swarm は<code class="language-plaintext highlighter-rouge">(N-1)/2</code>個のノードまでなら、それが完全に処理不能になっても耐えることができますが、これを越えると Swarm 管理を行うような要求は処理できなくなります。
ここで言う処理不能というのは、データ損傷やハードウェア障害などを表わします。</p>

<p>マネージャーが quorum を失った場合、Swarm を管理することはできません。
quorum を失った状態で、Swarm 上で管理操作を行おうとすると、以下のようなエラーが発生します。</p>

<pre><code class="language-none">Error response from daemon: rpc error: code = 4 desc = context deadline exceeded
</code></pre>

<p>quorum を失った状態から復旧するための最良の策は、障害が発生したノードをオンライン復旧させることです。
それが実現できない場合、この状態を回復する最後の手段は、マネージャーノード上において<code class="language-plaintext highlighter-rouge">--force-new-cluster</code>を実行することです。
これを行うと、コマンド実行したマネージャーノードを除いた、他のマネージャーノードがすべて削除されます。
マネージャーがたった 1 つになるのですから quorum は維持されます。
この後には、マネージャーノードが必要な数に達するまで、ノードをマネージャーに昇格させていきます。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># From the node to recover</span>
docker swarm init <span class="nt">--force-new-cluster</span> <span class="nt">--advertise-addr</span> node01:2377
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">docker swarm init</code>コマンドにおいて<code class="language-plaintext highlighter-rouge">--force-new-cluster</code>フラグをつけて実行すると、コマンド実行した Docker Engine は、単一ノードからなる Swarm のマネージャーノードとなり、サービスの管理と実行が可能となります。
このマネージャーには、それまでのサービスやタスクに関する情報がそのまま保持され、ワーカーノードは Swarm に参加したままです。
そしてサービスは実行を継続します。
タスク分散を実現するためには、マネージャーノードの追加あるいは再度の追加が必要になります。
そして高可用性と quorum を維持していくのに十分なマネージャーとなっていることを確認します。</p>

<h2 id="force-the-swarm-to-rebalance">Swarm の強制的な再配分</h2>

<p>一般には、Swarm におけるタスクを強制的に再配分させるような必要はありません。
Swarm に新たなノードを参加させたり、あるいは利用不能だったノードを Swarm に再接続させたりした場合に、Swarm ではアイドル状態にあるノードに対して、自動的な処理負荷の再配分は行いません。
これは設計方針によるものです。
仮に Swarm が配分を考えて、別のノードに定期的にタスクを振り替えるとすると、そのタスクを使用するクライアントは処理中断します。
目指すことは Swarm 内における配分を適切に行って、実行するサービスを中断させないことです。
新たなタスクが実行されたとき、あるいは実行中のタスクを持ったノードが利用不能になったとき、タスクは負荷の少ないノードに割り振られます。
最終的にはバランスよく配分されることです。
エンドユーザーに対して中断をできる限り少なくすることです。</p>

<p>Docker 1.13 およびそれ以降においては、<code class="language-plaintext highlighter-rouge">docker service update</code>コマンドに<code class="language-plaintext highlighter-rouge">--force</code>あるいは<code class="language-plaintext highlighter-rouge">-f</code>フラグをつけて実行することで、利用可能なワーカーノードに対して強制的にタスクを再分散します。
これを行うとサービスタスクは再起動します。
このときにはクライアントアプリケーションが中断するかもしれません。
設定によってサービスが <a href="/docs.docker.jp.onthefly/engine/swarm/swarm-tutorial/rolling-update/">ローリングアップデート</a> を利用するようにできます。</p>

<p>Docker の古いバージョンを利用していて、ワーカーノードに均等に負荷を配分したい場合、そしてタスクの中断が気にならない場合は、サービスのスケールを一時的に増やして、強制的に再配分を行うことができます。
設定されているサービスのスケール数は<code class="language-plaintext highlighter-rouge">docker service inspect --pretty &lt;servicename&gt;</code>を実行して確認します。
<code class="language-plaintext highlighter-rouge">docker service scale</code>を確認すれば、タスクの実行数が最も低いノードに、新たに配分が行われることがわかります。
Swarm 内にはもっと配分を負うことができるノードが複数見つかるかもしれません。
すべてのノードにわたってバランスよく配分を実現するためには、適度にサービススケールを増やすことを何度か行うことが必要かもしれません。</p>

<p>望んだとおりに負荷が配分されたら、サービスのスケールを元の値に戻します。
<code class="language-plaintext highlighter-rouge">docker service ps</code>を実行して、全ノード内でのサービスの配分状況を確認してください。</p>

<p><a href="/docs.docker.jp.onthefly/engine/reference/commandline/service_scale/"><code class="language-plaintext highlighter-rouge">docker service scale</code></a> と <a href="/docs.docker.jp.onthefly/engine/reference/commandline/service_ps/"><code class="language-plaintext highlighter-rouge">docker service ps</code></a> を参照してください。</p>
